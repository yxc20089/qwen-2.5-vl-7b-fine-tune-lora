{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Qwen2.5-VL-7B-Instruct on MathVerse Dataset (MLX - macOS)\n",
    "\n",
    "This notebook fine-tunes Qwen2.5-VL-7B-Instruct using MLX for Apple Silicon (M1/M2/M3/M4).\n",
    "\n",
    "**Requirements:**\n",
    "- macOS with Apple Silicon (M1/M2/M3/M4)\n",
    "- Python 3.9+\n",
    "- Sufficient RAM (32GB+ recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLX and required packages\n",
    "!pip install -q -U pip\n",
    "!pip install -q mlx mlx-lm\n",
    "!pip install -q transformers datasets accelerate peft pillow\n",
    "!pip install -q huggingface-hub\n",
    "!pip install -q qwen-vl-utils  # Qwen VL utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# MLX imports\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "# HuggingFace imports\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "print(f\"MLX version: {mx.__version__}\")\n",
    "print(f\"Device: Apple Silicon\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths - Update these to match your local paths\n",
    "DATA_DIR = \"/Users/berta/Documents/Projects/mathverse\"\n",
    "JSONL_PATH = f\"{DATA_DIR}/mathverse_testmini.jsonl\"\n",
    "IMAGES_DIR = f\"{DATA_DIR}/mathverse_testmini_images\"\n",
    "\n",
    "# Verify paths exist\n",
    "assert os.path.exists(JSONL_PATH), f\"JSONL file not found: {JSONL_PATH}\"\n",
    "assert os.path.exists(IMAGES_DIR), f\"Images directory not found: {IMAGES_DIR}\"\n",
    "\n",
    "print(f\"✅ Data directory: {DATA_DIR}\")\n",
    "print(f\"✅ JSONL file: {JSONL_PATH}\")\n",
    "print(f\"✅ Images directory: {IMAGES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONL data\n",
    "from PIL import ImageFile\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def clean_image_tokens(text):\n",
    "    \"\"\"Remove any existing image tokens from text.\"\"\"\n",
    "    patterns = [\n",
    "        r'<\\|image_pad\\|>',\n",
    "        r'<\\|vision_start\\|>',\n",
    "        r'<\\|vision_end\\|>',\n",
    "        r'<image>',\n",
    "        r'</image>',\n",
    "        r'\\[IMG\\d*\\]',\n",
    "    ]\n",
    "    cleaned = text\n",
    "    for pattern in patterns:\n",
    "        cleaned = re.sub(pattern, '', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def load_mathverse_data(jsonl_path, images_dir, max_samples=None):\n",
    "    \"\"\"\n",
    "    Load MathVerse dataset from JSONL file and images.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    errors = 0\n",
    "    image_errors = 0\n",
    "    \n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if max_samples and len(data) >= max_samples:\n",
    "                break\n",
    "            \n",
    "            if not line.strip():\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Skipping line {idx+1} due to JSON error: {e}\")\n",
    "                errors += 1\n",
    "                continue\n",
    "            \n",
    "            # Construct image path\n",
    "            image_path = os.path.join(images_dir, item['image_path'])\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Warning: Image not found: {image_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Get question and answer\n",
    "            question = clean_image_tokens(item.get('query', ''))\n",
    "            answer = item.get('answer', '')\n",
    "            \n",
    "            if not question or not answer:\n",
    "                print(f\"Warning: Skipping line {idx+1} - missing question or answer\")\n",
    "                continue\n",
    "            \n",
    "            # Load and verify image\n",
    "            try:\n",
    "                pil_image = Image.open(image_path).convert('RGB')\n",
    "                _ = pil_image.size\n",
    "            except (OSError, IOError) as e:\n",
    "                print(f\"Warning: Skipping line {idx+1} - corrupted image {image_path}: {e}\")\n",
    "                image_errors += 1\n",
    "                continue\n",
    "            \n",
    "            # Format in conversation format - image BEFORE text\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": pil_image},\n",
    "                        {\"type\": \"text\", \"text\": question}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": answer}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            data.append({\"messages\": conversation})\n",
    "    \n",
    "    print(f\"✅ Loaded {len(data)} samples\")\n",
    "    if errors > 0:\n",
    "        print(f\"⚠️ Skipped {errors} lines due to JSON errors\")\n",
    "    if image_errors > 0:\n",
    "        print(f\"⚠️ Skipped {image_errors} corrupted/truncated images\")\n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "raw_data = load_mathverse_data(\n",
    "    JSONL_PATH, \n",
    "    IMAGES_DIR,\n",
    "    max_samples=None  # Set to 100 for quick testing\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "if raw_data:\n",
    "    print(\"\\nSample data:\")\n",
    "    print(f\"Keys: {raw_data[0].keys()}\")\n",
    "    print(f\"Number of messages: {len(raw_data[0]['messages'])}\")\n",
    "    print(f\"Content order: {[c['type'] for c in raw_data[0]['messages'][0]['content']]}\")\n",
    "    print(f\"Question (first 200 chars): {raw_data[0]['messages'][0]['content'][1]['text'][:200]}...\")\n",
    "    print(f\"Answer: {raw_data[0]['messages'][1]['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    raw_data, \n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Keep as plain Python lists (don't convert to HF Dataset to preserve PIL Images)\n",
    "train_dataset = train_data\n",
    "val_dataset = val_data\n",
    "\n",
    "print(f\"\\n✅ Using plain Python lists\")\n",
    "print(f\"Image type check: {type(train_dataset[0]['messages'][0]['content'][0]['image'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with PyTorch (MLX doesn't support Qwen2-VL yet)\n",
    "\n",
    "**Note:** As of now, MLX doesn't have native Qwen2.5-VL support. We'll use PyTorch with MPS (Metal Performance Shaders) backend for Apple Silicon.\n",
    "\n",
    "For true MLX support, you would need to wait for MLX-VLM or convert the model manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For macOS, we'll use PyTorch with MPS backend\n",
    "import torch\n",
    "\n",
    "# Check for MPS availability\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✅ Using MPS (Metal Performance Shaders) backend\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"✅ Using CUDA backend\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ Using CPU backend\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "# Load model and processor\n",
    "# Note: We can't use 4-bit quantization on MPS, so loading in float16\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,  # Use float16 for memory efficiency\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LoRA for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"✅ LoRA applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "OUTPUT_DIR = \"./qwen2.5-vl-mathverse-finetuned-mlx\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Start with 1 for M4\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # Effective batch size = 8\n",
    "    \n",
    "    # Optimizer settings\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=5,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Precision - use fp16 for MPS\n",
    "    fp16=False,  # MPS doesn't support fp16 training yet\n",
    "    bf16=False,\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # Important for vision models\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # Other settings\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    "    \n",
    "    # Use MPS if available\n",
    "    use_mps_device=torch.backends.mps.is_available(),\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Device: {'MPS' if training_args.use_mps_device else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Data Collator for Vision Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data collator for vision-language models\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class VisionLanguageDataCollator:\n",
    "    \"\"\"Data collator for vision-language models.\"\"\"\n",
    "    processor: any\n",
    "    \n",
    "    def __call__(self, examples: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Process a batch of examples.\n",
    "        \"\"\"\n",
    "        # Extract images and create text prompts\n",
    "        images = []\n",
    "        texts = []\n",
    "        \n",
    "        for example in examples:\n",
    "            messages = example['messages']\n",
    "            \n",
    "            # Extract image from user message\n",
    "            for content_item in messages[0]['content']:\n",
    "                if content_item['type'] == 'image':\n",
    "                    images.append(content_item['image'])\n",
    "            \n",
    "            # Apply chat template\n",
    "            text_prompt = self.processor.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=False,\n",
    "                tokenize=False\n",
    "            )\n",
    "            texts.append(text_prompt)\n",
    "        \n",
    "        # Process with processor\n",
    "        batch = self.processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Set labels (same as input_ids for causal LM)\n",
    "        batch['labels'] = batch['input_ids'].clone()\n",
    "        \n",
    "        return batch\n",
    "\n",
    "data_collator = VisionLanguageDataCollator(processor=processor)\n",
    "print(\"✅ Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Trainer and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer initialized\")\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n✅ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "FINAL_MODEL_DIR = \"./qwen2.5-vl-mathverse-final-mlx\"\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(FINAL_MODEL_DIR)\n",
    "processor.save_pretrained(FINAL_MODEL_DIR)\n",
    "\n",
    "print(f\"✅ Model saved to {FINAL_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a validation sample\n",
    "model.eval()\n",
    "\n",
    "test_sample = val_data[0]\n",
    "test_image = test_sample['messages'][0]['content'][0]['image']\n",
    "test_question = test_sample['messages'][0]['content'][1]['text']\n",
    "expected_answer = test_sample['messages'][1]['content'][0]['text']\n",
    "\n",
    "# Prepare input\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": test_image},\n",
    "            {\"type\": \"text\", \"text\": test_question}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "text_prompt = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text_prompt],\n",
    "    images=[test_image],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "# Decode response\n",
    "generated_text = processor.batch_decode(\n",
    "    output,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")[0]\n",
    "\n",
    "print(\"Question:\")\n",
    "print(test_question[:200])\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(expected_answer)\n",
    "print(\"\\nModel Response:\")\n",
    "print(generated_text)\n",
    "print(\"\\nImage:\")\n",
    "display(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Full Evaluation (Optional)\n",
    "\n",
    "Run the same evaluation functions from the Colab version here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for MLX\n",
    "\n",
    "**Current Limitations:**\n",
    "- MLX doesn't yet have native Qwen2.5-VL support\n",
    "- This notebook uses PyTorch with MPS backend instead\n",
    "- MPS provides good performance on Apple Silicon\n",
    "\n",
    "**For True MLX Support:**\n",
    "1. Wait for MLX-VLM to add Qwen2.5-VL support\n",
    "2. Or manually convert the model using `mlx.utils.tree_map`\n",
    "3. Monitor: https://github.com/ml-explore/mlx-examples\n",
    "\n",
    "**Performance Tips:**\n",
    "- M4 Pro/Max: Can handle batch_size=2-4\n",
    "- M4: Start with batch_size=1\n",
    "- Increase gradient_accumulation_steps instead of batch size\n",
    "- Monitor memory usage in Activity Monitor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
