{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Qwen2.5-VL-7B-Instruct with MLX-VLM (Native MLX)\n",
    "\n",
    "This notebook uses **MLX-VLM** for native MLX fine-tuning on Apple Silicon.\n",
    "\n",
    "**Requirements:**\n",
    "- macOS with Apple Silicon (M1/M2/M3/M4)\n",
    "- Python 3.9+\n",
    "- 32GB+ RAM recommended\n",
    "\n",
    "**MLX-VLM Repository:** https://github.com/Blaizzy/mlx-vlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install MLX-VLM and dependencies\n",
    "!pip install -q -U pip\n",
    "!pip install -q mlx-vlm\n",
    "!pip install -q pillow datasets scikit-learn tqdm\n",
    "\n",
    "print(\"\u2705 Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLX version: 0.29.3\n",
      "Device: Apple Silicon\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageFile\n",
    "import numpy as np\n",
    "\n",
    "# MLX imports\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "# MLX-VLM imports\n",
    "from mlx_vlm import load, generate\n",
    "from mlx_vlm.utils import load_config\n",
    "\n",
    "print(f\"MLX version: {mx.__version__}\")\n",
    "print(f\"Device: Apple Silicon\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "mx.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Data directory: /Users/berta/Documents/Projects/mathvista\n",
      "\u2705 JSONL file: /Users/berta/Documents/Projects/mathvista/mathvista_testmini.jsonl\n",
      "\u2705 Images directory: /Users/berta/Documents/Projects/mathvista/mathvista_testmini_images\n"
     ]
    }
   ],
   "source": [
    "# Data paths - Update these to match your local paths\n",
    "DATA_DIR = \"/Users/berta/Documents/Projects/mathvista\"\n",
    "JSONL_PATH = f\"{DATA_DIR}/mathvista_testmini.jsonl\"\n",
    "IMAGES_DIR = f\"{DATA_DIR}/mathvista_testmini_images\"\n",
    "\n",
    "# Verify paths exist\n",
    "assert os.path.exists(JSONL_PATH), f\"JSONL file not found: {JSONL_PATH}\"\n",
    "assert os.path.exists(IMAGES_DIR), f\"Images directory not found: {IMAGES_DIR}\"\n",
    "\n",
    "print(f\"\u2705 Data directory: {DATA_DIR}\")\n",
    "print(f\"\u2705 JSONL file: {JSONL_PATH}\")\n",
    "print(f\"\u2705 Images directory: {IMAGES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Loaded 821 samples\n",
      "\n",
      "Sample data:\n",
      "Image: /Users/berta/Documents/Projects/mathvista/mathvista_testmini_images/question_0000.png\n",
      "Prompt: Hint: Please answer the question requiring a floating-point number with one decimal place and provide the final value, e.g., 1.2, 1.3, 1.4, at the end.\n",
      "Question: When a spring does work on an object, ...\n",
      "Answer: 1.2\n"
     ]
    }
   ],
   "source": [
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def clean_image_tokens(text):\n",
    "    \"\"\"Remove any existing image tokens from text.\"\"\"\n",
    "    patterns = [\n",
    "        r'<\\|image_pad\\|>',\n",
    "        r'<\\|vision_start\\|>',\n",
    "        r'<\\|vision_end\\|>',\n",
    "        r'<image>',\n",
    "        r'</image>',\n",
    "        r'\\[IMG\\d*\\]',\n",
    "    ]\n",
    "    cleaned = text\n",
    "    for pattern in patterns:\n",
    "        cleaned = re.sub(pattern, '', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def load_mathverse_data(jsonl_path, images_dir, max_samples=None):\n",
    "    \"\"\"\n",
    "    Load MathVerse dataset in MLX-VLM format.\n",
    "    \n",
    "    MLX-VLM expects data in format:\n",
    "    [\n",
    "        {\n",
    "            \"image\": \"path/to/image.jpg\",\n",
    "            \"prompt\": \"Question text\",\n",
    "            \"answer\": \"Answer text\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    errors = 0\n",
    "    image_errors = 0\n",
    "    \n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if max_samples and len(data) >= max_samples:\n",
    "                break\n",
    "            \n",
    "            if not line.strip():\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Skipping line {idx+1} due to JSON error\")\n",
    "                errors += 1\n",
    "                continue\n",
    "            \n",
    "            # Construct image path\n",
    "            image_path = os.path.join(images_dir, item['image_path'])\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "            \n",
    "            # Get question and answer\n",
    "            question = clean_image_tokens(item.get('query', ''))\n",
    "            answer = item.get('answer', '')\n",
    "            \n",
    "            if not question or not answer:\n",
    "                continue\n",
    "            \n",
    "            # Verify image can be loaded\n",
    "            try:\n",
    "                img = Image.open(image_path)\n",
    "                _ = img.size\n",
    "                img.close()\n",
    "            except (OSError, IOError):\n",
    "                image_errors += 1\n",
    "                continue\n",
    "            \n",
    "            # MLX-VLM format\n",
    "            data.append({\n",
    "                \"image\": image_path,\n",
    "                \"prompt\": question,\n",
    "                \"answer\": answer\n",
    "            })\n",
    "    \n",
    "    print(f\"\u2705 Loaded {len(data)} samples\")\n",
    "    if errors > 0:\n",
    "        print(f\"\u26a0\ufe0f Skipped {errors} lines due to JSON errors\")\n",
    "    if image_errors > 0:\n",
    "        print(f\"\u26a0\ufe0f Skipped {image_errors} corrupted images\")\n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "raw_data = load_mathverse_data(\n",
    "    JSONL_PATH, \n",
    "    IMAGES_DIR,\n",
    "    max_samples=None  # Set to 100 for quick testing\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "if raw_data:\n",
    "    print(\"\\nSample data:\")\n",
    "    print(f\"Image: {raw_data[0]['image']}\")\n",
    "    print(f\"Prompt: {raw_data[0]['prompt'][:200]}...\")\n",
    "    print(f\"Answer: {raw_data[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 738\n",
      "Validation samples: 83\n",
      "\n",
      "\u2705 Saved train.jsonl and val.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    raw_data, \n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Save to JSONL files for MLX-VLM\n",
    "with open('train.jsonl', 'w') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('val.jsonl', 'w') as f:\n",
    "    for item in val_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"\\n\u2705 Saved train.jsonl and val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with MLX-VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2.5-VL-7B-Instruct\n",
      "This may take a few minutes on first run...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da1926d8b0b4f6b8b19cb3df0cd5b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Model loaded successfully!\n",
      "Model type: <class 'mlx_vlm.models.qwen2_5_vl.qwen2_5_vl.Model'>\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "# Load model and processor\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "\n",
    "model, processor = load(MODEL_NAME)\n",
    "\n",
    "print(\"\u2705 Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Base Model (Before Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing base model...\n",
      "Question: Hint: Please answer the question requiring a floating-point number with two decimal places and provide the final value, e.g., 1.23, 1.34, 1.45, at the end.\n",
      "Question: What is the different between the ...\n",
      "Expected: 10.53\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The image Hint: Please answer the question requiring a floating-point number with two decimal places and provide the final value, e.g., 1.23, 1.34, 1.45, at the end.\nQuestion: What is the different between the highest unemployment rate and the lowest? must be a valid URL or existing file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_sample[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m response = \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBase Model Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mImage:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/experiment/fine-tune/.venv/lib/python3.11/site-packages/mlx_vlm/generate.py:532\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(model, processor, prompt, image, audio, verbose, **kwargs)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    530\u001b[39m     tokenizer.stopping_criteria.reset(model.config.eos_token_id)\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/experiment/fine-tune/.venv/lib/python3.11/site-packages/mlx_vlm/generate.py:399\u001b[39m, in \u001b[36mstream_generate\u001b[39m\u001b[34m(model, processor, prompt, image, audio, **kwargs)\u001b[39m\n\u001b[32m    397\u001b[39m     mask = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     inputs = \u001b[43mprepare_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_token_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_token_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresize_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresize_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m     input_ids = inputs.get(\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    409\u001b[39m     pixel_values = inputs.get(\u001b[33m\"\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/experiment/fine-tune/.venv/lib/python3.11/site-packages/mlx_vlm/utils.py:799\u001b[39m, in \u001b[36mprepare_inputs\u001b[39m\u001b[34m(processor, images, audio, prompts, image_token_index, resize_shape, add_special_tokens)\u001b[39m\n\u001b[32m    794\u001b[39m         images = [images]\n\u001b[32m    796\u001b[39m     image_processor = (\n\u001b[32m    797\u001b[39m         processor.image_processor \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(processor, \u001b[33m\"\u001b[39m\u001b[33mimage_processor\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    798\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m     images = \u001b[43m[\u001b[49m\u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# Process audio\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m audio:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/experiment/fine-tune/.venv/lib/python3.11/site-packages/mlx_vlm/utils.py:799\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    794\u001b[39m         images = [images]\n\u001b[32m    796\u001b[39m     image_processor = (\n\u001b[32m    797\u001b[39m         processor.image_processor \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(processor, \u001b[33m\"\u001b[39m\u001b[33mimage_processor\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    798\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m     images = [\u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# Process audio\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m audio:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/experiment/fine-tune/.venv/lib/python3.11/site-packages/mlx_vlm/utils.py:636\u001b[39m, in \u001b[36mprocess_image\u001b[39m\u001b[34m(img, resize_shape, image_processor)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_image\u001b[39m(img, resize_shape, image_processor):\n\u001b[32m    635\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m         img = \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resize_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_processor, BaseImageProcessor):\n\u001b[32m    638\u001b[39m         img = resize_image(img, resize_shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/experiment/fine-tune/.venv/lib/python3.11/site-packages/mlx_vlm/utils.py:618\u001b[39m, in \u001b[36mload_image\u001b[39m\u001b[34m(image_source, timeout)\u001b[39m\n\u001b[32m    614\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    615\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to load image from URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_source\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    616\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    619\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_source\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be a valid URL or existing file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    620\u001b[39m     )\n\u001b[32m    622\u001b[39m image = ImageOps.exif_transpose(image)\n\u001b[32m    623\u001b[39m image = image.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: The image Hint: Please answer the question requiring a floating-point number with two decimal places and provide the final value, e.g., 1.23, 1.34, 1.45, at the end.\nQuestion: What is the different between the highest unemployment rate and the lowest? must be a valid URL or existing file."
     ]
    }
   ],
   "source": [
    "# Test on a sample",
    "test_sample = val_data[0]",
    "test_image = Image.open(test_sample['image'])",
    "",
    "print(\"Testing base model...\")",
    "print(f\"Question: {test_sample['prompt'][:200]}...\")",
    "print(f\"Expected: {test_sample['answer']}\")",
    "",
    "# Generate response",
    "response = generate(",
    "    model,",
    "    processor,",
    "    test_sample['prompt'],",
    "    test_image,",
    "    max_tokens=128,",
    "    temp=0.0",
    ")",
    "",
    "print(f\"\\nBase Model Response: {response}\")",
    "print(\"\\nImage:\")",
    "display(test_image)",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-tune with LoRA using MLX-VLM CLI\n",
    "\n",
    "MLX-VLM provides a command-line interface for fine-tuning. We'll use it from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 1\n",
    "ITERS = 300  # Number of training iterations\n",
    "LEARNING_RATE = 1e-5\n",
    "LORA_RANK = 16\n",
    "ADAPTER_FILE = \"adapters.safetensors\"\n",
    "OUTPUT_DIR = \"./qwen2_5_vl_finetuned_mlx\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Iterations: {ITERS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  - Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MLX-VLM fine-tuning\n",
    "!mlx_vlm.lora \\\n",
    "    --model {MODEL_NAME} \\\n",
    "    --train \\\n",
    "    --data train.jsonl \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --iters {ITERS} \\\n",
    "    --learning-rate {LEARNING_RATE} \\\n",
    "    --lora-layers {LORA_RANK} \\\n",
    "    --adapter-file {ADAPTER_FILE} \\\n",
    "    --test-batches 10 \\\n",
    "    --val-data val.jsonl\n",
    "\n",
    "print(\"\\n\u2705 Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with adapters\n",
    "print(\"Loading fine-tuned model with adapters...\")\n",
    "\n",
    "# Reload model with adapters\n",
    "model, processor = load(\n",
    "    MODEL_NAME,\n",
    "    adapter_path=ADAPTER_FILE\n",
    ")\n",
    "\n",
    "print(\"\u2705 Fine-tuned model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on same sample",
    "test_sample = val_data[0]",
    "test_image = Image.open(test_sample['image'])",
    "",
    "print(\"Testing fine-tuned model...\")",
    "print(f\"Question: {test_sample['prompt'][:200]}...\")",
    "print(f\"Expected: {test_sample['answer']}\")",
    "",
    "# Generate response",
    "response = generate(",
    "    model,",
    "    processor,",
    "    test_sample['prompt'],",
    "    test_image,",
    "    max_tokens=128,",
    "    temp=0.0",
    ")",
    "",
    "print(f\"\\nFine-tuned Model Response: {response}\")",
    "print(\"\\nImage:\")",
    "display(test_image)",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fine-tuned model",
    "from tqdm.auto import tqdm",
    "",
    "def extract_answer(text, question_type='multi-choice'):",
    "    \"\"\"Extract answer from model output.\"\"\"",
    "    if question_type == 'multi-choice':",
    "        matches = re.findall(r'\\b([A-D])\\b', text.upper())",
    "        if matches:",
    "            return matches[-1]",
    "        return None",
    "    else:",
    "        return text.strip()",
    "",
    "def evaluate_model(model, processor, test_data, max_samples=None):",
    "    \"\"\"Evaluate model on test dataset.\"\"\"",
    "    results = []",
    "    correct = 0",
    "    total = 0",
    "",
    "    samples_to_eval = test_data[:max_samples] if max_samples else test_data",
    "",
    "    for idx, sample in enumerate(tqdm(samples_to_eval, desc=\"Evaluating\")):",
    "        try:",
    "            image = Image.open(sample['image'])",
    "            question = sample['prompt']",
    "            expected = sample['answer']",
    "",
    "            # Determine question type",
    "            question_type = 'multi-choice' if 'Choices:' in question else 'free-form'",
    "",
    "            # Generate response",
    "            response = generate(",
    "                model,",
    "                processor,",
    "                question,",
    "                image,",
    "                max_tokens=128,",
    "                temp=0.0",
    "            )",
    "",
    "            # Extract answer",
    "            predicted = extract_answer(response, question_type)",
    "",
    "            # Check correctness",
    "            is_correct = False",
    "            if question_type == 'multi-choice':",
    "                expected_letter = extract_answer(expected, 'multi-choice')",
    "                if expected_letter is None:",
    "                    expected_letter = expected.strip().upper()",
    "                is_correct = (predicted == expected_letter)",
    "            else:",
    "                is_correct = (expected.lower() in response.lower())",
    "",
    "            if is_correct:",
    "                correct += 1",
    "            total += 1",
    "",
    "            results.append({",
    "                'index': idx,",
    "                'expected': expected,",
    "                'predicted': predicted if predicted else response[:100],",
    "                'correct': is_correct",
    "            })",
    "",
    "        except Exception as e:",
    "            print(f\"Error on sample {idx}: {e}\")",
    "            continue",
    "",
    "    accuracy = (correct / total * 100) if total > 0 else 0",
    "    return results, accuracy",
    "",
    "# Run evaluation",
    "print(\"Evaluating fine-tuned model on validation set...\")",
    "results, accuracy = evaluate_model(",
    "    model,",
    "    processor,",
    "    val_data,",
    "    max_samples=None  # Set to 10 for quick test",
    ")",
    "",
    "print(f\"\\n\" + \"=\" * 80)",
    "print(f\"EVALUATION RESULTS\")",
    "print(\"=\" * 80)",
    "print(f\"Total Samples: {len(results)}\")",
    "print(f\"Correct: {sum(1 for r in results if r['correct'])}\")",
    "print(f\"Accuracy: {accuracy:.2f}%\")",
    "print(\"=\" * 80)",
    "",
    "# Save results",
    "with open('evaluation_results_mlx.json', 'w') as f:",
    "    json.dump({",
    "        'accuracy': accuracy,",
    "        'results': results",
    "    }, f, indent=2)",
    "",
    "print(\"\\n\u2705 Results saved to: evaluation_results_mlx.json\")",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save and Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The adapters are already saved in adapters.safetensors\n",
    "# To use the model later:\n",
    "\n",
    "print(\"Model artifacts:\")\n",
    "print(f\"  - Adapters: {ADAPTER_FILE}\")\n",
    "print(f\"  - Training data: train.jsonl\")\n",
    "print(f\"  - Validation data: val.jsonl\")\n",
    "print(f\"\\nTo load later:\")\n",
    "print(f\"  model, processor = load('{MODEL_NAME}', adapter_path='{ADAPTER_FILE}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fuse Adapters (Optional)\n",
    "\n",
    "You can merge the LoRA adapters into the base model for easier deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse adapters into base model\n",
    "!mlx_vlm.fuse \\\n",
    "    --model {MODEL_NAME} \\\n",
    "    --adapter-file {ADAPTER_FILE} \\\n",
    "    --save-path {OUTPUT_DIR}\n",
    "\n",
    "print(f\"\\n\u2705 Fused model saved to: {OUTPUT_DIR}\")\n",
    "print(f\"\\nTo use fused model:\")\n",
    "print(f\"  model, processor = load('{OUTPUT_DIR}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Performance on Apple Silicon\n",
    "\n",
    "**M4:**\n",
    "- Training speed: ~100-200 tokens/sec\n",
    "- Memory usage: ~15-20GB\n",
    "- Recommended: batch_size=1, iters=300\n",
    "\n",
    "**M4 Pro/Max:**\n",
    "- Training speed: ~200-400 tokens/sec\n",
    "- Memory usage: ~20-30GB\n",
    "- Can use larger batch sizes\n",
    "\n",
    "### Tips\n",
    "\n",
    "- **Reduce memory**: Lower `lora_rank` to 8 or use smaller `max_seq_length`\n",
    "- **Speed up**: Use smaller `max_samples` for testing\n",
    "- **Resume training**: Add `--resume-adapter-file` flag\n",
    "- **Quantization**: Use 4-bit quantized model for lower memory usage\n",
    "\n",
    "### Resources\n",
    "\n",
    "- MLX-VLM GitHub: https://github.com/Blaizzy/mlx-vlm\n",
    "- MLX Documentation: https://ml-explore.github.io/mlx/\n",
    "- Qwen2.5-VL: https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}