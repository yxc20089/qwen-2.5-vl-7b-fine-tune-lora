{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Qwen2.5-VL-7B-Instruct with MLX-VLM (Native MLX)\n",
    "\n",
    "This notebook uses **MLX-VLM** for native MLX fine-tuning on Apple Silicon.\n",
    "\n",
    "**Requirements:**\n",
    "- macOS with Apple Silicon (M1/M2/M3/M4)\n",
    "- Python 3.9+\n",
    "- 32GB+ RAM recommended\n",
    "\n",
    "**MLX-VLM Repository:** https://github.com/Blaizzy/mlx-vlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.\n",
      "torchmetrics 1.5.0 requires numpy<2.0,>1.20.0, but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m✅ Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install MLX-VLM and dependencies\n",
    "!pip install -q -U pip\n",
    "!pip install -q mlx-vlm\n",
    "!pip install -q pillow datasets scikit-learn tqdm\n",
    "\n",
    "print(\"✅ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLX version: 0.29.3\n",
      "Device: Apple Silicon\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageFile\n",
    "import numpy as np\n",
    "\n",
    "# MLX imports\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "# MLX-VLM imports\n",
    "from mlx_vlm import load, generate\n",
    "from mlx_vlm.utils import load_config\n",
    "\n",
    "print(f\"MLX version: {mx.__version__}\")\n",
    "print(f\"Device: Apple Silicon\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "mx.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data directory: /Users/berta/Documents/Projects/mathvista\n",
      "✅ JSONL file: /Users/berta/Documents/Projects/mathvista/mathvista_testmini.jsonl\n",
      "✅ Images directory: /Users/berta/Documents/Projects/mathvista/mathvista_testmini_images\n"
     ]
    }
   ],
   "source": [
    "# Data paths - Update these to match your local paths\n",
    "DATA_DIR = \"/Users/berta/Documents/Projects/mathvista\"\n",
    "JSONL_PATH = f\"{DATA_DIR}/mathvista_testmini.jsonl\"\n",
    "IMAGES_DIR = f\"{DATA_DIR}/mathvista_testmini_images\"\n",
    "\n",
    "# Verify paths exist\n",
    "assert os.path.exists(JSONL_PATH), f\"JSONL file not found: {JSONL_PATH}\"\n",
    "assert os.path.exists(IMAGES_DIR), f\"Images directory not found: {IMAGES_DIR}\"\n",
    "\n",
    "print(f\"✅ Data directory: {DATA_DIR}\")\n",
    "print(f\"✅ JSONL file: {JSONL_PATH}\")\n",
    "print(f\"✅ Images directory: {IMAGES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 821 samples\n",
      "\n",
      "Sample data:\n",
      "Image: /Users/berta/Documents/Projects/mathvista/mathvista_testmini_images/question_0000.png\n",
      "Prompt: Hint: Please answer the question requiring a floating-point number with one decimal place and provide the final value, e.g., 1.2, 1.3, 1.4, at the end.\n",
      "Question: When a spring does work on an object, ...\n",
      "Answer: 1.2\n"
     ]
    }
   ],
   "source": [
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def clean_image_tokens(text):\n",
    "    \"\"\"Remove any existing image tokens from text.\"\"\"\n",
    "    patterns = [\n",
    "        r'<\\|image_pad\\|>',\n",
    "        r'<\\|vision_start\\|>',\n",
    "        r'<\\|vision_end\\|>',\n",
    "        r'<image>',\n",
    "        r'</image>',\n",
    "        r'\\[IMG\\d*\\]',\n",
    "    ]\n",
    "    cleaned = text\n",
    "    for pattern in patterns:\n",
    "        cleaned = re.sub(pattern, '', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def load_mathverse_data(jsonl_path, images_dir, max_samples=None):\n",
    "    \"\"\"\n",
    "    Load MathVerse dataset in MLX-VLM format.\n",
    "    \n",
    "    MLX-VLM expects data in format:\n",
    "    [\n",
    "        {\n",
    "            \"image\": \"path/to/image.jpg\",\n",
    "            \"prompt\": \"Question text\",\n",
    "            \"answer\": \"Answer text\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    errors = 0\n",
    "    image_errors = 0\n",
    "    \n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if max_samples and len(data) >= max_samples:\n",
    "                break\n",
    "            \n",
    "            if not line.strip():\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Skipping line {idx+1} due to JSON error\")\n",
    "                errors += 1\n",
    "                continue\n",
    "            \n",
    "            # Construct image path\n",
    "            image_path = os.path.join(images_dir, item['image_path'])\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "            \n",
    "            # Get question and answer\n",
    "            question = clean_image_tokens(item.get('query', ''))\n",
    "            answer = item.get('answer', '')\n",
    "            \n",
    "            if not question or not answer:\n",
    "                continue\n",
    "            \n",
    "            # Verify image can be loaded\n",
    "            try:\n",
    "                img = Image.open(image_path)\n",
    "                _ = img.size\n",
    "                img.close()\n",
    "            except (OSError, IOError):\n",
    "                image_errors += 1\n",
    "                continue\n",
    "            \n",
    "            # MLX-VLM format\n",
    "            data.append({\n",
    "                \"image\": image_path,\n",
    "                \"prompt\": question,\n",
    "                \"answer\": answer\n",
    "            })\n",
    "    \n",
    "    print(f\"✅ Loaded {len(data)} samples\")\n",
    "    if errors > 0:\n",
    "        print(f\"⚠️ Skipped {errors} lines due to JSON errors\")\n",
    "    if image_errors > 0:\n",
    "        print(f\"⚠️ Skipped {image_errors} corrupted images\")\n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "raw_data = load_mathverse_data(\n",
    "    JSONL_PATH, \n",
    "    IMAGES_DIR,\n",
    "    max_samples=None  # Set to 100 for quick testing\n",
    ")\n",
    "\n",
    "# Display sample\n",
    "if raw_data:\n",
    "    print(\"\\nSample data:\")\n",
    "    print(f\"Image: {raw_data[0]['image']}\")\n",
    "    print(f\"Prompt: {raw_data[0]['prompt'][:200]}...\")\n",
    "    print(f\"Answer: {raw_data[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 738\n",
      "Validation samples: 83\n",
      "\n",
      "✅ Saved train.jsonl and val.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    raw_data, \n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Save to JSONL files for MLX-VLM\n",
    "with open('train.jsonl', 'w') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('val.jsonl', 'w') as f:\n",
    "    for item in val_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"\\n✅ Saved train.jsonl and val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with MLX-VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2.5-VL-7B-Instruct\n",
      "This may take a few minutes on first run...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e8dfc61349480bb0511a09cd7223ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6031f7c97fd04534b63e78604871c5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7930aeee8ddb426d989e968f71d5ad1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f633a1f259d4b6693ab4fc59278f60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2e1048c7ed4bb58e2cb3ef82b7c547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29239f472d9423496bdc6a3dbec27df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c62eefa2cd2414c96859d902fdb1a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae019c3634c4bfcabb7bdb2b519c3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc20e5af9c44f7881f2dd78753468cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0f4567db2544e6a6ce4ea40679518c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d67011c12a46d7824a4ddca19ab19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88f4bf778fc454fa8b2d9d366c75e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3344498ae563400986e1984b84ccb062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2762532be649c8ba7a027881446aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feeb9be34a4d4b988300a82c407060b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "# Load model and processor\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "\n",
    "model, processor = load(MODEL_NAME)\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Base Model (Before Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample\n",
    "test_sample = val_data[0]\n",
    "test_image = Image.open(test_sample['image'])\n",
    "\n",
    "print(\"Testing base model...\")\n",
    "print(f\"Question: {test_sample['prompt'][:200]}...\")\n",
    "print(f\"Expected: {test_sample['answer']}\")\n",
    "\n",
    "# Generate response\n",
    "response = generate(\n",
    "    model,\n",
    "    processor,\n",
    "    test_image,\n",
    "    test_sample['prompt'],\n",
    "    max_tokens=128,\n",
    "    temp=0.0\n",
    ")\n",
    "\n",
    "print(f\"\\nBase Model Response: {response}\")\n",
    "print(\"\\nImage:\")\n",
    "display(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-tune with LoRA using MLX-VLM CLI\n",
    "\n",
    "MLX-VLM provides a command-line interface for fine-tuning. We'll use it from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 1\n",
    "ITERS = 300  # Number of training iterations\n",
    "LEARNING_RATE = 1e-5\n",
    "LORA_RANK = 16\n",
    "ADAPTER_FILE = \"adapters.safetensors\"\n",
    "OUTPUT_DIR = \"./qwen2_5_vl_finetuned_mlx\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Iterations: {ITERS}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  - LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  - Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MLX-VLM fine-tuning\n",
    "!mlx_vlm.lora \\\n",
    "    --model {MODEL_NAME} \\\n",
    "    --train \\\n",
    "    --data train.jsonl \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --iters {ITERS} \\\n",
    "    --learning-rate {LEARNING_RATE} \\\n",
    "    --lora-layers {LORA_RANK} \\\n",
    "    --adapter-file {ADAPTER_FILE} \\\n",
    "    --test-batches 10 \\\n",
    "    --val-data val.jsonl\n",
    "\n",
    "print(\"\\n✅ Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with adapters\n",
    "print(\"Loading fine-tuned model with adapters...\")\n",
    "\n",
    "# Reload model with adapters\n",
    "model, processor = load(\n",
    "    MODEL_NAME,\n",
    "    adapter_path=ADAPTER_FILE\n",
    ")\n",
    "\n",
    "print(\"✅ Fine-tuned model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on same sample\n",
    "test_sample = val_data[0]\n",
    "test_image = Image.open(test_sample['image'])\n",
    "\n",
    "print(\"Testing fine-tuned model...\")\n",
    "print(f\"Question: {test_sample['prompt'][:200]}...\")\n",
    "print(f\"Expected: {test_sample['answer']}\")\n",
    "\n",
    "# Generate response\n",
    "response = generate(\n",
    "    model,\n",
    "    processor,\n",
    "    test_image,\n",
    "    test_sample['prompt'],\n",
    "    max_tokens=128,\n",
    "    temp=0.0\n",
    ")\n",
    "\n",
    "print(f\"\\nFine-tuned Model Response: {response}\")\n",
    "print(\"\\nImage:\")\n",
    "display(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fine-tuned model\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def extract_answer(text, question_type='multi-choice'):\n",
    "    \"\"\"Extract answer from model output.\"\"\"\n",
    "    if question_type == 'multi-choice':\n",
    "        matches = re.findall(r'\\b([A-D])\\b', text.upper())\n",
    "        if matches:\n",
    "            return matches[-1]\n",
    "        return None\n",
    "    else:\n",
    "        return text.strip()\n",
    "\n",
    "def evaluate_model(model, processor, test_data, max_samples=None):\n",
    "    \"\"\"Evaluate model on test dataset.\"\"\"\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    samples_to_eval = test_data[:max_samples] if max_samples else test_data\n",
    "\n",
    "    for idx, sample in enumerate(tqdm(samples_to_eval, desc=\"Evaluating\")):\n",
    "        try:\n",
    "            image = Image.open(sample['image'])\n",
    "            question = sample['prompt']\n",
    "            expected = sample['answer']\n",
    "\n",
    "            # Determine question type\n",
    "            question_type = 'multi-choice' if 'Choices:' in question else 'free-form'\n",
    "\n",
    "            # Generate response\n",
    "            response = generate(\n",
    "                model,\n",
    "                processor,\n",
    "                image,\n",
    "                question,\n",
    "                max_tokens=128,\n",
    "                temp=0.0\n",
    "            )\n",
    "\n",
    "            # Extract answer\n",
    "            predicted = extract_answer(response, question_type)\n",
    "\n",
    "            # Check correctness\n",
    "            is_correct = False\n",
    "            if question_type == 'multi-choice':\n",
    "                expected_letter = extract_answer(expected, 'multi-choice')\n",
    "                if expected_letter is None:\n",
    "                    expected_letter = expected.strip().upper()\n",
    "                is_correct = (predicted == expected_letter)\n",
    "            else:\n",
    "                is_correct = (expected.lower() in response.lower())\n",
    "\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            results.append({\n",
    "                'index': idx,\n",
    "                'expected': expected,\n",
    "                'predicted': predicted if predicted else response[:100],\n",
    "                'correct': is_correct\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on sample {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    accuracy = (correct / total * 100) if total > 0 else 0\n",
    "    return results, accuracy\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Evaluating fine-tuned model on validation set...\")\n",
    "results, accuracy = evaluate_model(\n",
    "    model,\n",
    "    processor,\n",
    "    val_data,\n",
    "    max_samples=None  # Set to 10 for quick test\n",
    ")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Samples: {len(results)}\")\n",
    "print(f\"Correct: {sum(1 for r in results if r['correct'])}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save results\n",
    "with open('evaluation_results_mlx.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'accuracy': accuracy,\n",
    "        'results': results\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n✅ Results saved to: evaluation_results_mlx.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save and Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The adapters are already saved in adapters.safetensors\n",
    "# To use the model later:\n",
    "\n",
    "print(\"Model artifacts:\")\n",
    "print(f\"  - Adapters: {ADAPTER_FILE}\")\n",
    "print(f\"  - Training data: train.jsonl\")\n",
    "print(f\"  - Validation data: val.jsonl\")\n",
    "print(f\"\\nTo load later:\")\n",
    "print(f\"  model, processor = load('{MODEL_NAME}', adapter_path='{ADAPTER_FILE}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fuse Adapters (Optional)\n",
    "\n",
    "You can merge the LoRA adapters into the base model for easier deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse adapters into base model\n",
    "!mlx_vlm.fuse \\\n",
    "    --model {MODEL_NAME} \\\n",
    "    --adapter-file {ADAPTER_FILE} \\\n",
    "    --save-path {OUTPUT_DIR}\n",
    "\n",
    "print(f\"\\n✅ Fused model saved to: {OUTPUT_DIR}\")\n",
    "print(f\"\\nTo use fused model:\")\n",
    "print(f\"  model, processor = load('{OUTPUT_DIR}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Performance on Apple Silicon\n",
    "\n",
    "**M4:**\n",
    "- Training speed: ~100-200 tokens/sec\n",
    "- Memory usage: ~15-20GB\n",
    "- Recommended: batch_size=1, iters=300\n",
    "\n",
    "**M4 Pro/Max:**\n",
    "- Training speed: ~200-400 tokens/sec\n",
    "- Memory usage: ~20-30GB\n",
    "- Can use larger batch sizes\n",
    "\n",
    "### Tips\n",
    "\n",
    "- **Reduce memory**: Lower `lora_rank` to 8 or use smaller `max_seq_length`\n",
    "- **Speed up**: Use smaller `max_samples` for testing\n",
    "- **Resume training**: Add `--resume-adapter-file` flag\n",
    "- **Quantization**: Use 4-bit quantized model for lower memory usage\n",
    "\n",
    "### Resources\n",
    "\n",
    "- MLX-VLM GitHub: https://github.com/Blaizzy/mlx-vlm\n",
    "- MLX Documentation: https://ml-explore.github.io/mlx/\n",
    "- Qwen2.5-VL: https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
