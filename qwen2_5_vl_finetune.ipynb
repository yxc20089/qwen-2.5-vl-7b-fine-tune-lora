{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Qwen2.5-VL-7B-Instruct on MathVerse Dataset\n",
    "\n",
    "This notebook fine-tunes the Qwen2.5-VL-7B-Instruct vision-language model using Unsloth for efficient training.\n",
    "\n",
    "**Important:** Make sure to enable GPU runtime (T4, V100, or A100) in Colab:\n",
    "- Runtime → Change runtime type → GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "**⚠️ IMPORTANT - First Time Setup:**\n",
    "\n",
    "If you're seeing KeyError about `align_logprobs_with_mask`:\n",
    "1. **Runtime → Factory reset runtime** (clears all packages)\n",
    "2. Then run the installation cell below\n",
    "3. After installation, manually restart: **Runtime → Restart runtime**\n",
    "4. Continue from cell 2 (imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Installation matching Unsloth official notebook\nimport os, re\nimport torch\n\n# Detect torch version for xformers compatibility\nv = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\nxformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n\n# First upgrade bitsandbytes\n!pip install -q -U bitsandbytes\n\n# Install dependencies without deps to avoid conflicts\n!pip install --no-deps -q accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n!pip install -q sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n\n# Install unsloth without deps\n!pip install --no-deps -q unsloth\n\n# Install compatible transformers and trl\n!pip install -q transformers\n!pip install --no-deps -q trl==0.22.2\n\nprint(\"\\n✅ Installation complete!\")\nprint(\"⚠️ Now restart runtime: Runtime → Restart session\")\nprint(\"After restart, skip this cell and run from imports\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries - IMPORTANT: Import unsloth FIRST\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import unsloth first for optimizations\n",
    "from unsloth import FastVisionModel, is_bfloat16_supported\n",
    "\n",
    "# Then import other libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, Features, Value, Image as DatasetImage\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    Qwen2VLForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Your Data to Colab\n",
    "\n",
    "You need to upload:\n",
    "1. `mathverse_testmini.jsonl`\n",
    "2. `mathverse_testmini_images/` folder (or zip it first)\n",
    "\n",
    "You can use Google Drive for large datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Mount Google Drive (recommended for large datasets)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Update these paths to match your Google Drive structure\n",
    "DATA_DIR = \"/content/drive/MyDrive/Colab Notebooks/data\"  # Updated based on your structure\n",
    "JSONL_PATH = f\"{DATA_DIR}/mathverse_testmini.jsonl\"\n",
    "IMAGES_DIR = f\"{DATA_DIR}/mathverse_testmini_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Upload files directly to Colab (for smaller datasets)\n",
    "# Uncomment if you prefer direct upload\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload your JSONL file\n",
    "\n",
    "# # Then extract images if uploaded as zip\n",
    "# !unzip -q mathverse_testmini_images.zip\n",
    "\n",
    "# DATA_DIR = \"/content\"\n",
    "# JSONL_PATH = f\"{DATA_DIR}/mathverse_testmini.jsonl\"\n",
    "# IMAGES_DIR = f\"{DATA_DIR}/mathverse_testmini_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load JSONL data\nimport json\nimport os\nimport re\nfrom PIL import Image, ImageFile\n\n# Allow loading of truncated images\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\ndef clean_image_tokens(text):\n    \"\"\"Remove any existing image tokens from text.\"\"\"\n    # Remove common vision tokens that might cause conflicts\n    patterns = [\n        r'<\\|image_pad\\|>',\n        r'<\\|vision_start\\|>',\n        r'<\\|vision_end\\|>',\n        r'<image>',\n        r'</image>',\n        r'\\[IMG\\d*\\]',\n    ]\n    cleaned = text\n    for pattern in patterns:\n        cleaned = re.sub(pattern, '', cleaned)\n    return cleaned.strip()\n\ndef load_mathverse_data(jsonl_path, images_dir, max_samples=None):\n    \"\"\"\n    Load MathVerse dataset from JSONL file and images.\n    Returns data in Unsloth conversation format with PIL Images.\n    \"\"\"\n    data = []\n    errors = 0\n    image_errors = 0\n    \n    with open(jsonl_path, 'r', encoding='utf-8') as f:\n        for idx, line in enumerate(f):\n            if max_samples and len(data) >= max_samples:\n                break\n            \n            # Skip empty lines\n            if not line.strip():\n                continue\n                \n            try:\n                item = json.loads(line)\n            except json.JSONDecodeError as e:\n                print(f\"Warning: Skipping line {idx+1} due to JSON error: {e}\")\n                errors += 1\n                continue\n            \n            # Construct image path\n            image_path = os.path.join(images_dir, item['image_path'])\n            \n            # Skip if image doesn't exist\n            if not os.path.exists(image_path):\n                print(f\"Warning: Image not found: {image_path}\")\n                continue\n            \n            # Get question and answer - clean any existing image tokens\n            question = clean_image_tokens(item.get('query', ''))\n            answer = item.get('answer', '')\n            \n            if not question or not answer:\n                print(f\"Warning: Skipping line {idx+1} - missing question or answer\")\n                continue\n            \n            # Load and verify image\n            try:\n                pil_image = Image.open(image_path).convert('RGB')\n                _ = pil_image.size  # Verify it loads\n            except (OSError, IOError) as e:\n                print(f\"Warning: Skipping line {idx+1} - corrupted image {image_path}: {e}\")\n                image_errors += 1\n                continue\n            \n            # Format exactly like official Unsloth notebook - image BEFORE text\n            conversation = [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"image\", \"image\": pil_image},\n                        {\"type\": \"text\", \"text\": question}\n                    ]\n                },\n                {\n                    \"role\": \"assistant\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": answer}\n                    ]\n                }\n            ]\n            \n            # Store ONLY messages, no separate images key\n            data.append({\"messages\": conversation})\n    \n    print(f\"✅ Loaded {len(data)} samples\")\n    if errors > 0:\n        print(f\"⚠️ Skipped {errors} lines due to JSON errors\")\n    if image_errors > 0:\n        print(f\"⚠️ Skipped {image_errors} corrupted/truncated images\")\n    return data\n\n# Load data (set max_samples to a number for testing, None loads full dataset)\nraw_data = load_mathverse_data(\n    JSONL_PATH, \n    IMAGES_DIR,\n    max_samples=None  # Set to 100 for quick testing, None for full dataset\n)\n\n# Display sample\nif raw_data:\n    print(\"\\nSample data:\")\n    print(f\"Keys: {raw_data[0].keys()}\")\n    print(f\"Number of messages: {len(raw_data[0]['messages'])}\")\n    print(f\"Content order: {[c['type'] for c in raw_data[0]['messages'][0]['content']]}\")\n    print(f\"Question (first 200 chars): {raw_data[0]['messages'][0]['content'][1]['text'][:200]}...\")\n    print(f\"Answer: {raw_data[0]['messages'][1]['content'][0]['text']}\")\nelse:\n    print(\"\\n❌ Error: No data loaded. Please check your data files.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split data into train and validation sets\nfrom sklearn.model_selection import train_test_split\n\ntrain_data, val_data = train_test_split(\n    raw_data, \n    test_size=0.1,  # 10% for validation\n    random_state=42\n)\n\nprint(f\"Training samples: {len(train_data)}\")\nprint(f\"Validation samples: {len(val_data)}\")\n\n# DO NOT convert to HF Dataset - keep as plain Python lists\n# HuggingFace Dataset.from_list() converts PIL Images to dicts which breaks Unsloth\ntrain_dataset = train_data  # Keep as list\nval_dataset = val_data      # Keep as list\n\nprint(f\"\\n✅ Using plain Python lists (not HF Dataset)\")\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Val samples: {len(val_dataset)}\")\n\n# Verify images are still PIL Images\nprint(f\"Image type check: {type(train_dataset[0]['messages'][0]['content'][0]['image'])}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LOAD_IN_4BIT = True  # Set to False if you have enough VRAM\n",
    "\n",
    "# Load model and processor with Unsloth\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    "    dtype=None,  # Auto-detect\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LoRA for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA using Unsloth\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Use Unsloth's optimized gradient checkpointing\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is already prepared in Unsloth format\n",
    "# No additional preprocessing needed - Unsloth will handle it\n",
    "print(\"✅ Dataset ready for training\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration\nfrom unsloth import UnslothTrainingArguments\nfrom unsloth.trainer import UnslothVisionDataCollator\n\nOUTPUT_DIR = \"./qwen2.5-vl-mathverse-finetuned\"\n\ntraining_args = UnslothTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    \n    # Training hyperparameters - start with batch size 1 to debug\n    num_train_epochs=3,\n    per_device_train_batch_size=1,  # Reduce to 1 to avoid batching issues\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,  # Increase to maintain effective batch size = 8\n    \n    # Optimizer settings\n    learning_rate=2e-5,\n    warmup_steps=5,\n    weight_decay=0.01,\n    \n    # Precision and performance\n    bf16=is_bfloat16_supported(),\n    fp16=not is_bfloat16_supported(),\n    optim=\"adamw_8bit\",  # Memory-efficient optimizer\n    \n    # Logging and evaluation\n    logging_steps=1,\n    eval_strategy=\"steps\",\n    eval_steps=20,\n    save_strategy=\"steps\",\n    save_steps=20,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    \n    # Important for vision models\n    remove_unused_columns=False,\n    dataset_text_field=\"\",\n    dataset_kwargs={\"skip_prepare_dataset\": True},\n    \n    # Other settings\n    report_to=\"none\",  # Change to \"wandb\" if you want to use Weights & Biases\n    seed=42,\n)\n\nprint(\"Training configuration:\")\nprint(f\"  - Epochs: {training_args.num_train_epochs}\")\nprint(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  - Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  - Learning rate: {training_args.learning_rate}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Unsloth's vision data collator\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "\n",
    "data_collator = UnslothVisionDataCollator(model, processor)\n",
    "print(\"✅ Using UnslothVisionDataCollator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with Unsloth's SFTTrainer\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "FINAL_MODEL_DIR = \"./qwen2.5-vl-mathverse-final\"\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(FINAL_MODEL_DIR)\n",
    "processor.save_pretrained(FINAL_MODEL_DIR)\n",
    "\n",
    "print(f\"Model saved to {FINAL_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save to Google Drive\n",
    "!cp -r {FINAL_MODEL_DIR} /content/drive/MyDrive/\n",
    "print(\"Model copied to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Merge LoRA Weights (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights with base model for easier deployment\n",
    "# Warning: This requires more memory\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "MERGED_MODEL_DIR = \"./qwen2.5-vl-mathverse-merged\"\n",
    "\n",
    "merged_model.save_pretrained(MERGED_MODEL_DIR)\n",
    "processor.save_pretrained(MERGED_MODEL_DIR)\n",
    "\n",
    "print(f\"Merged model saved to {MERGED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test on a single validation sample (quick test)\nfrom PIL import Image\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Test on a validation sample\ntest_sample = val_data[0]\n# Extract image and question from our conversation format\ntest_image = test_sample['messages'][0]['content'][0]['image']  # Image is first in content\ntest_question = test_sample['messages'][0]['content'][1]['text']  # Text is second\nexpected_answer = test_sample['messages'][1]['content'][0]['text']\n\n# Prepare input\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": test_image},\n            {\"type\": \"text\", \"text\": test_question}\n        ]\n    }\n]\n\ntext_prompt = processor.apply_chat_template(\n    conversation,\n    add_generation_prompt=True,\n    tokenize=False\n)\n\ninputs = processor(\n    text=[text_prompt],\n    images=[test_image],\n    return_tensors=\"pt\",\n    padding=True\n).to(model.device)\n\n# Generate response\nwith torch.no_grad():\n    output = model.generate(\n        **inputs,\n        max_new_tokens=128,\n        do_sample=False,\n        temperature=0.7,\n        top_p=0.9\n    )\n\n# Decode response\ngenerated_text = processor.batch_decode(\n    output,\n    skip_special_tokens=True,\n    clean_up_tokenization_spaces=True\n)[0]\n\nprint(\"Question:\")\nprint(test_question[:200])\nprint(\"\\nExpected Answer:\")\nprint(expected_answer)\nprint(\"\\nModel Response:\")\nprint(generated_text)\nprint(\"\\nImage:\")\ndisplay(test_image)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. Full Dataset Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Full Test Dataset Evaluation - PURE VISION (No Text At All)\nimport torch\nimport json\nimport re\nfrom tqdm.auto import tqdm\n\ndef extract_answer(text, question_type='multi-choice'):\n    \"\"\"\n    Extract answer from model output.\n    For multi-choice: look for single letter A, B, C, D\n    For free-form: return the full text\n    \"\"\"\n    if question_type == 'multi-choice':\n        # Look for patterns like \"A\", \"B\", \"C\", \"D\" (case insensitive)\n        # Try to find the last occurrence to get the final answer\n        matches = re.findall(r'\\b([A-D])\\b', text.upper())\n        if matches:\n            return matches[-1]  # Return last match\n        return None\n    else:\n        # For free-form, return cleaned text\n        return text.strip()\n\ndef evaluate_model_pure_vision(model, processor, test_data, max_samples=None):\n    \"\"\"\n    Evaluate model on test dataset - PURE VISION (NO TEXT AT ALL).\n    Tests if model can answer based on image alone with no text prompt.\n    \"\"\"\n    model.eval()\n    results = []\n    correct = 0\n    total = 0\n\n    # Limit samples if specified\n    samples_to_eval = test_data[:max_samples] if max_samples else test_data\n\n    print(f\"Evaluating on {len(samples_to_eval)} samples (PURE VISION - NO TEXT)...\")\n\n    for idx, sample in enumerate(tqdm(samples_to_eval, desc=\"Evaluating\")):\n        # Extract data from conversation format\n        test_image = sample['messages'][0]['content'][0]['image']\n        test_question = sample['messages'][0]['content'][1]['text']  # Keep for reference only\n        expected_answer = sample['messages'][1]['content'][0]['text']\n\n        # Determine question type\n        question_type = 'multi-choice' if 'Choices:' in test_question else 'free-form'\n\n        # Prepare input - IMAGE ONLY, NO TEXT AT ALL\n        conversation = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": test_image}\n                ]\n            }\n        ]\n\n        text_prompt = processor.apply_chat_template(\n            conversation,\n            add_generation_prompt=True,\n            tokenize=False\n        )\n\n        inputs = processor(\n            text=[text_prompt],\n            images=[test_image],\n            return_tensors=\"pt\",\n            padding=True\n        ).to(model.device)\n\n        # Generate response\n        try:\n            with torch.no_grad():\n                output = model.generate(\n                    **inputs,\n                    max_new_tokens=128,\n                    do_sample=False,\n                    temperature=0.0,  # Greedy decoding for evaluation\n                )\n\n            # Decode response\n            generated_text = processor.batch_decode(\n                output,\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=True\n            )[0]\n\n            # Extract answer\n            predicted_answer = extract_answer(generated_text, question_type)\n\n            # Check correctness\n            is_correct = False\n            if question_type == 'multi-choice':\n                expected_letter = extract_answer(expected_answer, 'multi-choice')\n                if expected_letter is None:\n                    expected_letter = expected_answer.strip().upper()\n                is_correct = (predicted_answer == expected_letter)\n            else:\n                is_correct = (expected_answer.lower() in generated_text.lower())\n\n            if is_correct:\n                correct += 1\n            total += 1\n\n            # Store result\n            results.append({\n                'index': idx,\n                'original_question': test_question[:200] + '...' if len(test_question) > 200 else test_question,\n                'expected_answer': expected_answer,\n                'predicted_answer': predicted_answer if predicted_answer else generated_text[:100],\n                'full_response': generated_text,\n                'question_type': question_type,\n                'correct': is_correct\n            })\n\n        except Exception as e:\n            print(f\"\\nError on sample {idx}: {e}\")\n            results.append({\n                'index': idx,\n                'error': str(e),\n                'correct': False\n            })\n\n        # Print progress every 10 samples\n        if (idx + 1) % 10 == 0:\n            current_acc = (correct / total * 100) if total > 0 else 0\n            print(f\"\\nProgress: {idx + 1}/{len(samples_to_eval)} | Accuracy: {current_acc:.2f}%\")\n\n    # Calculate metrics\n    accuracy = (correct / total * 100) if total > 0 else 0\n    mc_correct = sum(1 for r in results if r.get('question_type') == 'multi-choice' and r['correct'])\n    mc_total = sum(1 for r in results if r.get('question_type') == 'multi-choice')\n    ff_correct = sum(1 for r in results if r.get('question_type') == 'free-form' and r['correct'])\n    ff_total = sum(1 for r in results if r.get('question_type') == 'free-form')\n\n    metrics = {\n        'total_samples': total,\n        'correct': correct,\n        'accuracy': accuracy,\n        'multi_choice_accuracy': (mc_correct / mc_total * 100) if mc_total > 0 else 0,\n        'multi_choice_count': mc_total,\n        'free_form_accuracy': (ff_correct / ff_total * 100) if ff_total > 0 else 0,\n        'free_form_count': ff_total\n    }\n\n    return results, metrics\n\n# Run PURE VISION evaluation\nprint(\"=\" * 80)\nprint(\"PURE VISION EVALUATION (NO TEXT - IMAGE ONLY)\")\nprint(\"=\" * 80)\nprint(\"Model receives ONLY the image, no text prompt at all\")\nprint(\"=\" * 80)\n\nresults_pure_vision, metrics_pure_vision = evaluate_model_pure_vision(\n    model,\n    processor,\n    val_data,\n    max_samples=None  # Set to 10 for quick test, None for full evaluation\n)\n\n# Print metrics\nprint(\"\\n\" + \"=\" * 80)\nprint(\"PURE VISION RESULTS\")\nprint(\"=\" * 80)\nprint(f\"Total Samples: {metrics_pure_vision['total_samples']}\")\nprint(f\"Correct: {metrics_pure_vision['correct']}\")\nprint(f\"Overall Accuracy: {metrics_pure_vision['accuracy']:.2f}%\")\nprint(f\"\\nMulti-Choice: {metrics_pure_vision['multi_choice_accuracy']:.2f}% ({metrics_pure_vision['multi_choice_count']} samples)\")\nprint(f\"Free-Form: {metrics_pure_vision['free_form_accuracy']:.2f}% ({metrics_pure_vision['free_form_count']} samples)\")\nprint(\"=\" * 80)\n\n# Save results\noutput_file = \"./evaluation_results_pure_vision.json\"\nwith open(output_file, 'w') as f:\n    json.dump({'metrics': metrics_pure_vision, 'results': results_pure_vision}, f, indent=2)\n\nprint(f\"\\n✅ Results saved to: {output_file}\")\n\n# Show examples\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SAMPLE PREDICTIONS (First 5)\")\nprint(\"=\" * 80)\nfor i, result in enumerate(results_pure_vision[:5]):\n    if 'original_question' in result:\n        print(f\"\\nSample {i+1}:\")\n        print(f\"Original Question (NOT shown to model): {result['original_question']}\")\n        print(f\"Expected: {result['expected_answer']}\")\n        print(f\"Predicted: {result['predicted_answer']}\")\n        print(f\"Full Response: {result['full_response'][:150]}...\")\n        print(f\"Correct: {'✓' if result['correct'] else '✗'}\")\n        print(\"-\" * 40)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 14. Compare Base Model vs Fine-tuned Model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Load Original Base Model for Comparison\nprint(\"=\" * 80)\nprint(\"LOADING ORIGINAL BASE MODEL (No Fine-tuning)\")\nprint(\"=\" * 80)\n\n# Free up memory first\nimport gc\nimport torch\n\n# Clear current model from GPU\nif 'model' in globals():\n    del model\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Load the original base model (without LoRA)\nfrom unsloth import FastVisionModel\n\nbase_model, base_processor = FastVisionModel.from_pretrained(\n    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n    max_seq_length=2048,\n    load_in_4bit=True,\n    dtype=None,\n)\n\nbase_model.eval()\n\nprint(\"✅ Base model loaded successfully!\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate Base Model (Pure Vision)\nprint(\"=\" * 80)\nprint(\"EVALUATING BASE MODEL (Pure Vision)\")\nprint(\"=\" * 80)\n\nresults_base, metrics_base = evaluate_model_pure_vision(\n    base_model,\n    base_processor,\n    val_data,\n    max_samples=None  # Set to 10 for quick test, None for full evaluation\n)\n\n# Print base model metrics\nprint(\"\\n\" + \"=\" * 80)\nprint(\"BASE MODEL RESULTS\")\nprint(\"=\" * 80)\nprint(f\"Total Samples: {metrics_base['total_samples']}\")\nprint(f\"Correct: {metrics_base['correct']}\")\nprint(f\"Overall Accuracy: {metrics_base['accuracy']:.2f}%\")\nprint(f\"\\nMulti-Choice: {metrics_base['multi_choice_accuracy']:.2f}% ({metrics_base['multi_choice_count']} samples)\")\nprint(f\"Free-Form: {metrics_base['free_form_accuracy']:.2f}% ({metrics_base['free_form_count']} samples)\")\nprint(\"=\" * 80)\n\n# Save base model results\noutput_file_base = \"./evaluation_results_base_model.json\"\nwith open(output_file_base, 'w') as f:\n    json.dump({'metrics': metrics_base, 'results': results_base}, f, indent=2)\n\nprint(f\"\\n✅ Base model results saved to: {output_file_base}\")"
  },
  {
   "cell_type": "code",
   "source": "# Reload Fine-tuned Model\nprint(\"=\" * 80)\nprint(\"RELOADING FINE-TUNED MODEL\")\nprint(\"=\" * 80)\n\n# Free up base model\ndel base_model\ndel base_processor\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Reload fine-tuned model with LoRA weights\nfrom unsloth import FastVisionModel\nfrom peft import PeftModel\n\nfinetuned_model, finetuned_processor = FastVisionModel.from_pretrained(\n    model_name=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n    max_seq_length=2048,\n    load_in_4bit=True,\n    dtype=None,\n)\n\n# Apply LoRA configuration\nfinetuned_model = FastVisionModel.get_peft_model(\n    finetuned_model,\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=42,\n)\n\n# Load the saved fine-tuned weights\nFINAL_MODEL_DIR = \"./qwen2.5-vl-mathverse-final\"\nfinetuned_model = PeftModel.from_pretrained(finetuned_model, FINAL_MODEL_DIR)\nfinetuned_model.eval()\n\nprint(\"✅ Fine-tuned model reloaded successfully!\")\nprint(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Evaluate Fine-tuned Model (Pure Vision)\nprint(\"=\" * 80)\nprint(\"EVALUATING FINE-TUNED MODEL (Pure Vision)\")\nprint(\"=\" * 80)\n\nresults_finetuned, metrics_finetuned = evaluate_model_pure_vision(\n    finetuned_model,\n    finetuned_processor,\n    val_data,\n    max_samples=None\n)\n\n# Print fine-tuned metrics\nprint(\"\\n\" + \"=\" * 80)\nprint(\"FINE-TUNED MODEL RESULTS\")\nprint(\"=\" * 80)\nprint(f\"Total Samples: {metrics_finetuned['total_samples']}\")\nprint(f\"Correct: {metrics_finetuned['correct']}\")\nprint(f\"Overall Accuracy: {metrics_finetuned['accuracy']:.2f}%\")\nprint(f\"\\nMulti-Choice: {metrics_finetuned['multi_choice_accuracy']:.2f}% ({metrics_finetuned['multi_choice_count']} samples)\")\nprint(f\"Free-Form: {metrics_finetuned['free_form_accuracy']:.2f}% ({metrics_finetuned['free_form_count']} samples)\")\nprint(\"=\" * 80)\n\n# Save results\nwith open('./evaluation_results_finetuned_model.json', 'w') as f:\n    json.dump({'metrics': metrics_finetuned, 'results': results_finetuned}, f, indent=2)\n\nprint(f\"\\n✅ Results saved\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Side-by-Side Comparison\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COMPARISON: BASE vs FINE-TUNED\")\nprint(\"=\" * 80)\n\nimport pandas as pd\n\n# Create comparison table\ncomparison_data = {\n    'Metric': [\n        'Overall Accuracy',\n        'Multi-Choice Accuracy',\n        'Free-Form Accuracy',\n        'Total Samples',\n        'Correct Answers'\n    ],\n    'Base Model': [\n        f\"{metrics_base['accuracy']:.2f}%\",\n        f\"{metrics_base['multi_choice_accuracy']:.2f}%\",\n        f\"{metrics_base['free_form_accuracy']:.2f}%\",\n        metrics_base['total_samples'],\n        metrics_base['correct']\n    ],\n    'Fine-Tuned Model': [\n        f\"{metrics_finetuned['accuracy']:.2f}%\",\n        f\"{metrics_finetuned['multi_choice_accuracy']:.2f}%\",\n        f\"{metrics_finetuned['free_form_accuracy']:.2f}%\",\n        metrics_finetuned['total_samples'],\n        metrics_finetuned['correct']\n    ],\n    'Improvement': [\n        f\"{metrics_finetuned['accuracy'] - metrics_base['accuracy']:+.2f}%\",\n        f\"{metrics_finetuned['multi_choice_accuracy'] - metrics_base['multi_choice_accuracy']:+.2f}%\",\n        f\"{metrics_finetuned['free_form_accuracy'] - metrics_base['free_form_accuracy']:+.2f}%\",\n        '-',\n        f\"{metrics_finetuned['correct'] - metrics_base['correct']:+d}\"\n    ]\n}\n\ndf_comparison = pd.DataFrame(comparison_data)\nprint(df_comparison.to_string(index=False))\nprint(\"\\n\" + \"=\" * 80)\n\n# Relative improvement\nif metrics_base['accuracy'] > 0:\n    relative_improvement = ((metrics_finetuned['accuracy'] - metrics_base['accuracy']) / metrics_base['accuracy']) * 100\n    print(f\"\\nRelative Improvement: {relative_improvement:+.2f}%\")\n\n# Save comparison\ncomparison_output = {\n    'base_model': metrics_base,\n    'finetuned_model': metrics_finetuned,\n    'improvement': {\n        'overall_accuracy': metrics_finetuned['accuracy'] - metrics_base['accuracy'],\n        'multi_choice_accuracy': metrics_finetuned['multi_choice_accuracy'] - metrics_base['multi_choice_accuracy'],\n        'correct_answers': metrics_finetuned['correct'] - metrics_base['correct']\n    }\n}\n\nwith open('./comparison_results.json', 'w') as f:\n    json.dump(comparison_output, f, indent=2)\n\nprint(\"✅ Comparison saved to: ./comparison_results.json\")\n\n# Examples where fine-tuning helped\nprint(\"\\n\" + \"=\" * 80)\nprint(\"EXAMPLES WHERE FINE-TUNING HELPED\")\nprint(\"=\" * 80)\n\nhelped_count = 0\nfor i in range(len(results_base)):\n    if not results_base[i].get('correct', False) and results_finetuned[i].get('correct', False):\n        helped_count += 1\n        if helped_count <= 3:\n            print(f\"\\nExample {helped_count}:\")\n            print(f\"Question: {results_base[i].get('original_question', '')[:150]}...\")\n            print(f\"Expected: {results_base[i].get('expected_answer', '')}\")\n            print(f\"Base: {results_base[i].get('predicted_answer', '')} ✗\")\n            print(f\"Fine-Tuned: {results_finetuned[i].get('predicted_answer', '')} ✓\")\n            print(\"-\" * 40)\n\nprint(f\"\\n✅ Fine-tuning helped on {helped_count} questions!\")\n\n# Regressions\nprint(\"\\n\" + \"=\" * 80)\nprint(\"REGRESSIONS\")\nprint(\"=\" * 80)\n\nregression_count = 0\nfor i in range(len(results_base)):\n    if results_base[i].get('correct', False) and not results_finetuned[i].get('correct', False):\n        regression_count += 1\n        if regression_count <= 3:\n            print(f\"\\nRegression {regression_count}:\")\n            print(f\"Question: {results_base[i].get('original_question', '')[:150]}...\")\n            print(f\"Expected: {results_base[i].get('expected_answer', '')}\")\n            print(f\"Base: {results_base[i].get('predicted_answer', '')} ✓\")\n            print(f\"Fine-Tuned: {results_finetuned[i].get('predicted_answer', '')} ✗\")\n            print(\"-\" * 40)\n\nprint(f\"\\n⚠️ {regression_count} regressions\")\nprint(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}